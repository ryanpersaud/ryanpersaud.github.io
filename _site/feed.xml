<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://0.0.0.0:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://0.0.0.0:4000/" rel="alternate" type="text/html" /><updated>2025-10-26T21:38:29+00:00</updated><id>http://0.0.0.0:4000/feed.xml</id><title type="html">Ryan Persaud’s blog</title><subtitle>This is apparently where all of my musings have ended up.</subtitle><entry><title type="html">Air Quality Micro Block</title><link href="http://0.0.0.0:4000/iot/hardware/2025/10/26/air-quality-micro-block.html" rel="alternate" type="text/html" title="Air Quality Micro Block" /><published>2025-10-26T18:39:11+00:00</published><updated>2025-10-26T18:39:11+00:00</updated><id>http://0.0.0.0:4000/iot/hardware/2025/10/26/air-quality-micro-block</id><content type="html" xml:base="http://0.0.0.0:4000/iot/hardware/2025/10/26/air-quality-micro-block.html"><![CDATA[<p>A few week’s ago, I accompanied Erin to her 25th reunion at Colorado College in Colorado Springs. They are one of a few schools that have the block plan where instead of taking multiple classes throughout a semester, students take one class at a time for three and a half weeks. This means that students have concentrated deep-dives into subjects - it definitely seems to resonate with some students more than others. I’m still not sure if it would suit my learning style. I like to deep dive on various topics, but I also recognize that sometimes I need time to internalize material. During the reunion weekend, Micro Blocks were offered where attendees could get quick exposure to a subject. I decided to take a Micro Block on Air Quality, and it ended up being quite interesting.</p>

<p>The Micro Block was led by a <a href="https://www.coloradocollege.edu/basics/contact/directory/people/bruder_andrea_s.html">Dr. Andrea Bruder</a>, a professor of Mathematics and Computer Science. Like many folks, she got interested in Air Quality during the pandemic. Rather than trying to regurgitate the content verbatim, I am going to focus on the areas that I found notable.</p>

<div style="text-align: center; margin-bottom: 2em;">
<img src="/assets/images/filter-petri-dish.jpg" alt="A bear coding with good vibes" style="width: 52.5%;" />
<p style="font-style: italic; color: #666; margin-top: 0.5em; font-size: 0.9em;">A sample taken from an indoor air filter and grown in a petri dish</p>
</div>

<p>Dr. Bruder swabbed an air filter from an HVAC system and grew the sample on agar (see above picture). It should not have been a surprise, but it was kind of shocking to see what grew out of normal indoor air. Mara did a similar experiment recently where she swabbed classmates’ and teachers’ computers and saw what grew from the samples. The results in that case were a little more understandable since it’s easy to piture all the bacteria living on a keyboard.</p>

<div style="text-align: center; margin-bottom: 2em;">
<img src="/assets/images/outdoor-air-reading.jpg" alt="A bear coding with good vibes" style="width: 70%;" />
<p style="font-style: italic; color: #666; margin-top: 0.5em; font-size: 0.9em;">Taking outdoor readings</p>
</div>

<p>It was helpful to review what PM2.5 (particulate matter 2.5) means (I often forget the precise definition). It’s a measurement of the particles that are 2.5 micrometers or smaller in the air. The first thing that I typically forget is that it’s not just particles that are 2.5 um in diameter, it’s &lt;= 2.5μm. It’s also important to note that rather than a count, PM2.5 is a measurement of the mass of such particles in a given volume, specifically μg/m3. One implication of this definition is that PM2.5 value depends on not just the number of particles in the air, but also the mass of the particles. Most Indoor Air Quality (IAQ) devices that I had previously used had PM2.5 and maybe PM1, but during the Micro Block, we got to use some devices that measured the count of particles of various sizes.</p>

<div style="text-align: center; margin-bottom: 2em;">
<img src="/assets/images/corsi-rosenthal-pc-fans.jpg" alt="A bear coding with good vibes" style="width: 52.5%;" />
<p style="font-style: italic; color: #666; margin-top: 0.5em; font-size: 0.9em;">Corsi-Rosenthal box in with PC fans in the classroom</p>
</div>

<p>During the pandemic, <a href="https://en.wikipedia.org/wiki/Corsi%E2%80%93Rosenthal_Box">Corsi-Rosenthal boxes</a> were a popular, relatively inexpensive way to filter the air in spaces like classrooms to remove Covid virus particles. I had heard of them and seen a few different models online, but I did not realize that Jim Rosenthal was a Colorado College alum. The design has continued to evolve and new models now use PC fans instead of box fans to move air through the box (see above). PC fans are quieter and more engery efficient than box fans, though box fans can generally move more air. Corsi-Rosenthal boxes use less energy and are quieter than HEPA filters, and, since they can move a large quantity of air per minute, they can still <a href="https://www.texairfilters.com/could-corsi-rosenthal-boxes-reduce-particles-to-cleanroom-levels/">get to ISO clean room standards</a> (with multiple air changes per hour). Jim Rosenthal will be co-teachning a <a href="https://today.coloradocollege.edu/announcements/32948">half block course</a> with Dr. Bruder on Environmental Health &amp; Indoor Air.</p>

<div style="text-align: center; margin-bottom: 2em;">
<img src="/assets/images/particle-size.png" alt="A bear coding with good vibes" style="width: 52.5%;" />
<p style="font-style: italic; color: #666; margin-top: 0.5em; font-size: 0.9em;">Fractional collection efficiency versus particle diameter for a mechanical filter (<a href="https://www.cdc.gov/niosh/docs/2003-136/pdfs/2003-136.pdf">source</a>)</p>
</div>

<p>Perhaps the most interesting fact I learned during the session was about particles that are .3μ in diameter. Couterintuitively, they are harder for most filters to capture than particles with smaller diameters since smaller particles exhibit <a href="https://en.wikipedia.org/wiki/Brownian_motion">Brownian motion</a> and the zig-zag paths that they traverse make them more likely to be trapped by filters.</p>]]></content><author><name></name></author><category term="IoT" /><category term="Hardware" /><category term="air-quality" /><category term="micro-block" /><summary type="html"><![CDATA[A few week’s ago, I accompanied Erin to her 25th reunion at Colorado College in Colorado Springs. They are one of a few schools that have the block plan where instead of taking multiple classes throughout a semester, students take one class at a time for three and a half weeks. This means that students have concentrated deep-dives into subjects - it definitely seems to resonate with some students more than others. I’m still not sure if it would suit my learning style. I like to deep dive on various topics, but I also recognize that sometimes I need time to internalize material. During the reunion weekend, Micro Blocks were offered where attendees could get quick exposure to a subject. I decided to take a Micro Block on Air Quality, and it ended up being quite interesting.]]></summary></entry><entry><title type="html">My First Foray into Vibe Coding</title><link href="http://0.0.0.0:4000/programming/2025/05/10/my-first-foray-into-vibe-coding.html" rel="alternate" type="text/html" title="My First Foray into Vibe Coding" /><published>2025-05-10T15:58:20+00:00</published><updated>2025-05-10T15:58:20+00:00</updated><id>http://0.0.0.0:4000/programming/2025/05/10/my-first-foray-into-vibe-coding</id><content type="html" xml:base="http://0.0.0.0:4000/programming/2025/05/10/my-first-foray-into-vibe-coding.html"><![CDATA[<div style="text-align: center; margin-bottom: 2em;">
<img src="/assets/images/bear-vibe-code.jpeg" alt="A bear coding with good vibes" style="width: 35%;" />
<p style="font-style: italic; color: #666; margin-top: 0.5em; font-size: 0.9em;">Created with Gemini using the prompt: Create an image of a bear acting like a human and vibe coding. The screen should contain go source code that is accessing dynamodb. Make it clear that the bear is not working too hard, but is instructing the LLM on how to program.</p>
</div>

<p>Based on all the chatter I’ve been hearing about vibe coding, I’m late to the party. I don’t use AI much in my day-to-day life - my primary use case is generating images like the one at the top of this post to troll my coworkers. However, I do recognize that in the near future (now?) most jobs are going to require some fluency with AI and there will be an expectation that AI tools are used to boost productivity. To get my feet wet with a coding assistant, I decided to use vibe coding to assist me in evaluating a change that we made to some retention logic. The steps I needed to perform are as follows:</p>

<ol>
  <li>Retrieve a binary blob from DynamoDB using an id as the key. The key could have one of two prefixes, depending on the environment. Also, the table that the blob is stored in could have one of two names, depending on the environment.</li>
  <li>Deserialize the binary blob into one of two structures, again depending on the environment.</li>
  <li>Iterate through the structure, looking for arrays of a particular structure.</li>
  <li>For any non-nil elements in the array, check whether the timestamp contained in the structure is less than a year old.</li>
</ol>

<div style="text-align: center; margin: 1em 0;">
<img src="/assets/images/blob.png" alt="Binary blob visualization" style="width: 25%;" />
<p style="font-style: italic; color: #666; margin-top: 0.5em; font-size: 0.9em;">Created with Gemini using the prompt: Create a cartoony image of a binary blob running away from DynamoDB</p>
</div>

<p>Up until now, when I had to do something similar, I would just use the AWS command line utility to dump the data from DynamoDB and then copy the data into a Go test case to do the deserializing and pretty printing as JSON (basically, steps 1 and 2). I figured I could replicate this process with some vibe coding, and then manually do steps 3 and 4 by piping the output into a chain of command line tools.</p>

<p>I had a pretty good idea of what this program should look like, and I described the task using phrasing similar to the steps above with some additional detail related to the tables and structures. I was pleasantly surprised that the generated code was almost completely correct on the first try. One bug was due to me providing an incorrect table name, and another one was due to the code attempting to use the <code class="language-plaintext highlighter-rouge">S</code> (string) value in the returned DynamoDB item instead of the <code class="language-plaintext highlighter-rouge">B</code> (binary) value. More impressive is that I didn’t have to explain which prefix to use for each environment; the system was able to infer it from other code in the repository. Things were going so swimmingly that I vibe coded steps 3 and 4 as well. The only major hiccup was that the program did not initially check for nil array elements for step 4, so the first iteration blew up. That was easy enough to fix, and I soon had a program that completely solved my task. I even added green check mark emoji next to every timestamp that was checked and passed the age test.</p>

<p>I cleaned up the base version of this program that handled steps 1 and 2, added pretty printed JSON output, and added it to the repository as a tool for other folks to use. I should note that the generated code had nice input checking and meaningful error messages which made it easy to determine what the issue was when the program failed.</p>

<p>I’ve since used AI a bit more to assist me in some development tasks, and it’s definitely been hit or miss. One task that is somewhat meta was to develop a container for Jekyll server so that I could preview my blog without polluting my local machine with various Ruby packages. The AI I was using got into a loop where it kept switching between adding and removing ARM-specific directives to the Dockerfile. I eventually just used <a href="https://github.com/BretFisher/jekyll-serve">this project</a> as a basis for the Dockerfile.</p>

<p>Also, I’m actually writing this blog post with some AI assistance. I used it to create the skeleton of the blog post and to provide some syntax guidance as I get used to Jekyll (it turns out that a lot of it is just the Markdown syntax that we’re all familiar with). I also had it do a spellcheck pass before I published the post.  I did not have it generate any content, as I want to keep my own voice/humanity.  I did run into a funny issue where I asked the AI assistant to update the date of the blog post (I started this about a week ago) to May 10, 2025, and it correctly changed the date portion of the <code class="language-plaintext highlighter-rouge">date</code> field, but it chose a time in the future, so Jekyll skipped processing the post.  I had to dig into the GitHub Actions output to see what was going on.  This is a somewhat subtle issue that could be tough to debug without some knowledge of the environment.</p>

<p>My main takeaway from my recent experiences is that LLMs can definitely be useful in speeding up some coding tasks - especially when the vibe programmer has a well-conceived idea of what the program should look like, the dependencies involved, and the gotchas that might crop up. For example, if someone had never worked with DynamoDB before, and they ran into the <code class="language-plaintext highlighter-rouge">S</code> vs <code class="language-plaintext highlighter-rouge">B</code> issue, they might be stuck (though maybe they could have talked it through with the LLM).</p>]]></content><author><name></name></author><category term="Programming" /><category term="vibe-coding" /><summary type="html"><![CDATA[Created with Gemini using the prompt: Create an image of a bear acting like a human and vibe coding. The screen should contain go source code that is accessing dynamodb. Make it clear that the bear is not working too hard, but is instructing the LLM on how to program.]]></summary></entry><entry><title type="html">Periodically Blocking Domains with dnscrypt-proxy</title><link href="http://0.0.0.0:4000/2023/05/11/blocking-domains-with-dnscrypt-proxy.html" rel="alternate" type="text/html" title="Periodically Blocking Domains with dnscrypt-proxy" /><published>2023-05-11T17:05:10+00:00</published><updated>2023-05-11T17:05:10+00:00</updated><id>http://0.0.0.0:4000/2023/05/11/blocking-domains-with-dnscrypt-proxy</id><content type="html" xml:base="http://0.0.0.0:4000/2023/05/11/blocking-domains-with-dnscrypt-proxy.html"><![CDATA[<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<!-- wp:paragraph --><html><body></p>
<p>I have the kids on their own wireless network/VLAN with DHCP that hands out a local DNS server that is really a container on my Pi running <a href="https://github.com/DNSCrypt/dnscrypt-proxy">dnscrypt-proxy</a>.  dnscrypt-proxy sends requests to a Cloudflare server that I’ve <a href="https://developers.cloudflare.com/cloudflare-one/policies/filtering/initial-setup/dns/">configured with various filtering options</a> (social media is blocked, youtube.com is a CNAME for restrict.youtube.com, etc.).  I wanted to restrict use of streaming services from 7PM-6AM at the network level, and I initially thought I’d have to replace dnscrypt-proxy with  a full blown DNS server that could blackhole the streaming services during ‘off-hours.’  Then, I thought maybe dnscrypt-proxy would support domain overrides and I could use a cron job to add and remove override entries as needed.  It turns out it’s even easier than that - dnsycrypt-proxy<a href="https://github.com/DNSCrypt/dnscrypt-proxy/blob/d381af5510879ad407fd23c407c05103da1c6269/dnscrypt-proxy/example-blocked-names.txt#L42"> supports a time-based blocklist out of the box</a>!</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>Here’s how I configured it:</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>I added the following schedule to my <code>dnscrypt-proxy.toml</code>:</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p><br><code>[schedules.'time-to-sleep']<br>mon = [{after='2:00', before='13:00'}]<br>tue = [{after='2:00', before='13:00'}]<br>wed = [{after='2:00', before='13:00'}]<br>thu = [{after='2:00', before='13:00'}]<br>fri = [{after='2:00', before='13:00'}]<br>sat = [{after='2:00', before='13:00'}]<br>sun = [{after='2:00', before='13:00'}]</code></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p><code>[blocked_names]<br>blocked_names_file = 'blocked-names.txt'</code></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>And the contents of <code>blocked-names.txt</code>:</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p><br><code>*.netflix.* @time-to-sleep<br>*.disney*. @time-to-sleep</code></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>This worked great, but shortly after deploying it I had this exchange:</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p><br>Child: Hmm, I can’t play my show on Netflix?<br></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>Me: Ha Ha! I blocked streaming after 7PM!<br></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>Child: Oh, my previously downloaded shows work, I’ll just watch one of those.<br></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>Me: ...</p>
<p><!-- /wp:paragraph --><br />
</body></html></p>]]></content><author><name></name></author><summary type="html"><![CDATA[I have the kids on their own wireless network/VLAN with DHCP that hands out a local DNS server that is really a container on my Pi running dnscrypt-proxy.  dnscrypt-proxy sends requests to a Cloudflare server that I’ve configured with various filtering options (social media is blocked, youtube.com is a CNAME for restrict.youtube.com, etc.).  I wanted to restrict use of streaming services from 7PM-6AM at the network level, and I initially thought I’d have to replace dnscrypt-proxy with  a full blown DNS server that could blackhole the streaming services during ‘off-hours.’  Then, I thought maybe dnscrypt-proxy would support domain overrides and I could use a cron job to add and remove override entries as needed.  It turns out it’s even easier than that - dnsycrypt-proxy supports a time-based blocklist out of the box! Here’s how I configured it: I added the following schedule to my dnscrypt-proxy.toml: [schedules.'time-to-sleep']mon = [{after='2:00', before='13:00'}]tue = [{after='2:00', before='13:00'}]wed = [{after='2:00', before='13:00'}]thu = [{after='2:00', before='13:00'}]fri = [{after='2:00', before='13:00'}]sat = [{after='2:00', before='13:00'}]sun = [{after='2:00', before='13:00'}] [blocked_names]blocked_names_file = 'blocked-names.txt' And the contents of blocked-names.txt: *.netflix.* @time-to-sleep*.disney*. @time-to-sleep This worked great, but shortly after deploying it I had this exchange: Child: Hmm, I can’t play my show on Netflix? Me: Ha Ha! I blocked streaming after 7PM! Child: Oh, my previously downloaded shows work, I’ll just watch one of those. Me: ...]]></summary></entry><entry><title type="html">Trying out IAM Access Analyzer policy generation (a year late)</title><link href="http://0.0.0.0:4000/2022/04/08/trying-out-iam-access-analyzer-policy-generation-a-year-late.html" rel="alternate" type="text/html" title="Trying out IAM Access Analyzer policy generation (a year late)" /><published>2022-04-08T20:25:11+00:00</published><updated>2022-04-08T20:25:11+00:00</updated><id>http://0.0.0.0:4000/2022/04/08/trying-out-iam-access-analyzer-policy-generation-a-year-late</id><content type="html" xml:base="http://0.0.0.0:4000/2022/04/08/trying-out-iam-access-analyzer-policy-generation-a-year-late.html"><![CDATA[<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<!-- wp:paragraph --><html><body></p>
<p>Since early on in my journey with AWS, I’ve been looking for tools to assist me in creating least privilege policies.  Many online examples and actual tool documentation provide overly permissive policies, so it’s important to analyze them before deploying them in production (or even DEV, depending on the permission!).  One tool that I experimented with a few years ago was DUO Labs’ <a href="https://github.com/duo-labs/cloudtracker">CloudTracker</a>.  I used it with another engineer, <a href="https://harryeldridge.com/">Harry</a>, to try and reduce the set of S3 permissions granted to GitLab’s role since the documentation recommended (at the time, at least) granting <code>S3:*</code>.  I also used it to get 90% of the permissions for a Terraform role that engineers could assume when applying their TF*.</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>So when AWS <a href="https://aws.amazon.com/blogs/security/iam-access-analyzer-makes-it-easier-to-implement-least-privilege-permissions-by-generating-iam-policies-based-on-access-activity/">announced IAM Access Analyzer policy generation</a> I was excited to try it out.  Not having to install and run an external tool and worry about my Athena spend was definitely attractive. A year later, and  I finally got around to trying it out this week at work, so I figured I’d write a brief post describing my experience.  Note, the example provided in this post is from my personal AWS account.</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>First, I created a role, <code>lambda-aa-test</code>,  with an inline policy that granted the role permission to call <code>GetFunction</code>:</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:code --></p>
<pre class="wp-block-code"><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "VisualEditor0",
            "Effect": "Allow",
            "Action": "lambda:GetFunction",
            "Resource": "*"
        }
    ]
}</code></pre>
<p><!-- /wp:code --></p>
<p><!-- wp:paragraph --></p>
<p>Then I assumed <code>lambda-aa-test</code> and called <code>get-function</code> on a Lambda function in my account.  Interestingly, CloudTrail records this as <code>GetFunction20150331v2</code> which caused some confusion when I was attempting to verify that the call had propagated to CloudTrail.  Next, I clicked the button to ‘Generate Policy’ and waited a couple of minutes while the analyzer did its thing.  Here’s the generated policy:</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:code --></p>
<pre class="wp-block-code"><code>{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": "sts:GetCallerIdentity",
            "Resource": "*"
        },
        {
        "Effect": "Allow",
            "Action": "lambda:GetFunction",
            "Resource": "arn:aws:lambda:${Region}:${Account}:function:${FunctionName}"
        }
    ]
}</code></pre>
<p><!-- /wp:code --></p>
<p><!-- wp:paragraph --></p>
<p><code>sts:GetCallerIdentity</code> is not a mistake - I did call that after assuming the role to ensure I was who I thought I was.  And you can see that <code>lambda:GetFunction</code> was also included as expected.  However, the resource is a templated ARN that can be filled in by the user.  This is useful for understanding what Resource the Action can be applied to (and the expected ARN format), but it isn’t super helpful in cases where there are a number of possible resources and we want to narrow down which ones an action can be performed on.  For example, if I had twenty Lambda functions, but I only ever call <code>GetFunction</code> on <code>MyFunction</code>, this generated policy would not help me limit <code>GetFunction </code>to <code>MyFunction</code>.  IIRC, this was also a  limitation of CloudTracker when I used it.</p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p><!-- /wp:paragraph --></p>
<p><!-- wp:paragraph --></p>
<p>* I should note that even with the reduced permissions, an attacker could still escalate their privileges by using the terraform role to create a policy with whatever permissions they desired.  <a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html">Permission Boundaries</a> can limit the effective permissions of policies created by the terraform role, but implementing PBs can be complex and difficult to debug.</p>
<p><!-- /wp:paragraph --><br />
</body></html></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Since early on in my journey with AWS, I’ve been looking for tools to assist me in creating least privilege policies.  Many online examples and actual tool documentation provide overly permissive policies, so it’s important to analyze them before deploying them in production (or even DEV, depending on the permission!).  One tool that I experimented with a few years ago was DUO Labs’ CloudTracker.  I used it with another engineer, Harry, to try and reduce the set of S3 permissions granted to GitLab’s role since the documentation recommended (at the time, at least) granting S3:*.  I also used it to get 90% of the permissions for a Terraform role that engineers could assume when applying their TF*. So when AWS announced IAM Access Analyzer policy generation I was excited to try it out.  Not having to install and run an external tool and worry about my Athena spend was definitely attractive. A year later, and  I finally got around to trying it out this week at work, so I figured I’d write a brief post describing my experience.  Note, the example provided in this post is from my personal AWS account. First, I created a role, lambda-aa-test,  with an inline policy that granted the role permission to call GetFunction: { "Version": "2012-10-17", "Statement": [ { "Sid": "VisualEditor0", "Effect": "Allow", "Action": "lambda:GetFunction", "Resource": "*" } ] } Then I assumed lambda-aa-test and called get-function on a Lambda function in my account.  Interestingly, CloudTrail records this as GetFunction20150331v2 which caused some confusion when I was attempting to verify that the call had propagated to CloudTrail.  Next, I clicked the button to ‘Generate Policy’ and waited a couple of minutes while the analyzer did its thing.  Here’s the generated policy: { "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": "sts:GetCallerIdentity", "Resource": "*" }, { "Effect": "Allow", "Action": "lambda:GetFunction", "Resource": "arn:aws:lambda:${Region}:${Account}:function:${FunctionName}" } ] } sts:GetCallerIdentity is not a mistake - I did call that after assuming the role to ensure I was who I thought I was.  And you can see that lambda:GetFunction was also included as expected.  However, the resource is a templated ARN that can be filled in by the user.  This is useful for understanding what Resource the Action can be applied to (and the expected ARN format), but it isn’t super helpful in cases where there are a number of possible resources and we want to narrow down which ones an action can be performed on.  For example, if I had twenty Lambda functions, but I only ever call GetFunction on MyFunction, this generated policy would not help me limit GetFunction to MyFunction.  IIRC, this was also a  limitation of CloudTracker when I used it. * I should note that even with the reduced permissions, an attacker could still escalate their privileges by using the terraform role to create a policy with whatever permissions they desired.  Permission Boundaries can limit the effective permissions of policies created by the terraform role, but implementing PBs can be complex and difficult to debug.]]></summary></entry><entry><title type="html">Creating a Home Testbed, and Kicking the Tires on JA3</title><link href="http://0.0.0.0:4000/2017/08/08/creating-a-home-testbed-and-kicking-the-tires-on-ja3.html" rel="alternate" type="text/html" title="Creating a Home Testbed, and Kicking the Tires on JA3" /><published>2017-08-08T03:36:27+00:00</published><updated>2017-08-08T03:36:27+00:00</updated><id>http://0.0.0.0:4000/2017/08/08/creating-a-home-testbed-and-kicking-the-tires-on-ja3</id><content type="html" xml:base="http://0.0.0.0:4000/2017/08/08/creating-a-home-testbed-and-kicking-the-tires-on-ja3.html"><![CDATA[<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body>
<p>For a while now, I have wanted the ability to easily sniff my own packets.  Specifically, I've wanted to be able to monitor all traffic entering and exiting my home network.  In the past, I've turned old machines into transparent bridges and wired them in between my router and cable modem.  However, actually using these boxes was never quite as simple as I had hoped, and in the back of my mind I was always afraid that the bridge would be detected and hax0red.  I knew the easiest solution would be to upgrade from my army of cheap, unmanaged switches to a more expensive manged switch that supported port mirroring.  In July, I finally ordered a <a href="https://www.amazon.com/TP-Link-24-Port-Gigabit-Ethernet-TL-SG1024DE/dp/B00CUG8ESM">TP-Link switch</a> that fit the bill.  I installed it the day it arrived, but I have not had a chance to try the more advanced features until this past weekend.  I figured I'd use the opportunity to kick the tires on the JA3 <a href="https://engineering.salesforce.com/open-sourcing-ja3-92c9e53c3c41">SSL/TLS client fingerprinting scheme</a> that some researchers at Salesforce have been working on.</p>
<p>Out of the box, the switch has the IP address 192.168.0.1, so I had to put a machine on that network in order to configure the switch.  Word to the wise, updating the firmware reverts the switch's IP address to 192.168.0.1, so you may want to update before changing the IP.  Anyway, <a href="http://www.tp-link.com/us/faq-527.html">enabling port mirroring</a> on the TP Link was straightforward, if inelegant (you get what you pay for).  One gotcha is that I had to select the mirroring port a second time before it 'stuck' (see previous parenthetical note).  I mirrored the port that connects the switch to my router, and after some futzing with my Raspberry Pi's external USB network adapter and an ancient USB hub, I was in business:</p>
<p><figure><img src="/assets/images/sniffer.jpeg?w=300" alt="" width="300" height="225"><figcaption>This setup is temporary and I hope to neaten it up at some point in the near future.</figcaption></figure></p>
<p>JA3 parses client Hello SSL/TLS packets and extracts out fields to create a string that can be used to uniquely identify a client (a fingerprint).  Instead of passing around a potentially large string, an MD5 hash is calculated.  Since I only needed to capture Hello packets, I did a quick Google to find a tcpdump filter that would drop the rest of the traffic coming over the mirror port:  <code>tcpdump -ni eth1 "tcp port 443 and (tcp[((tcp[12] &amp; 0xf0) &gt;&gt; 2)] = 0x16)"</code></p>
<p>This filter actually grabs server Hellos too, but I can live with that for now.  I let the capture run for about a day, and then I ran the <a href="https://github.com/salesforce/ja3/tree/master/python">JA3 python script</a> on the capture file to generate the aforementioned hashes.  Even though the capture file was only 39MB, it took the script a while to process all of the packets.  At some point, I'd like to take a closer look at the script and see if there are any opportunities for optimization.  In order to make my life easier, I did add a JSON output option and submit a <a href="https://github.com/salesforce/ja3/pull/5">Pull Request</a>.  Once I had my fingerprints, I compared them against <a href="https://github.com/trisulnsm/trisul-scripts/blob/master/lua/frontend_scripts/reassembly/ja3/prints/ja3fingerprint.json">a list that the trisul folks generated in late June</a> from <a href="https://github.com/LeeBrotherston/tls-fingerprinting/blob/master/fingerprints/fingerprints.json">fingerprint strings</a> that Lee Brotherston generated in late March.  Anyway, the point is that these fingerprints are not that recent - and it showed.  Of 82 unique fingerprints that were captured in my network, seven of them appeared in the trisul list.  None of the seven were Chrome which makes sense given the high release cadence that the Chrome team maintains.  I am curious what is lurking in those 75 remaining hashes.  My Nest?  Angry Birds?  Jinyang's fridge?</p>
<p>So, the first thought I have is that for JA3 to be useful in a whitelisting approach, there is going to have to be a trusted source of fingerprints that is constantly profiling new versions of popular clients and generating fingerprints for them.  It would be nice to have a process that could monitor for new versions of applications like Chrome and Firefox, automatically pull them down, and generate signatures for them.  The second thought I had is that there might be less unique fingerprints to deal with in an enterprise setting (depending on how locked down the network is, of course) since there aren't going to be a bunch of random devices phoning home and mobile apps being used.</p>
<p></body></html></p>]]></content><author><name></name></author><summary type="html"><![CDATA[For a while now, I have wanted the ability to easily sniff my own packets.  Specifically, I've wanted to be able to monitor all traffic entering and exiting my home network.  In the past, I've turned old machines into transparent bridges and wired them in between my router and cable modem.  However, actually using these boxes was never quite as simple as I had hoped, and in the back of my mind I was always afraid that the bridge would be detected and hax0red.  I knew the easiest solution would be to upgrade from my army of cheap, unmanaged switches to a more expensive manged switch that supported port mirroring.  In July, I finally ordered a TP-Link switch that fit the bill.  I installed it the day it arrived, but I have not had a chance to try the more advanced features until this past weekend.  I figured I'd use the opportunity to kick the tires on the JA3 SSL/TLS client fingerprinting scheme that some researchers at Salesforce have been working on. Out of the box, the switch has the IP address 192.168.0.1, so I had to put a machine on that network in order to configure the switch.  Word to the wise, updating the firmware reverts the switch's IP address to 192.168.0.1, so you may want to update before changing the IP.  Anyway, enabling port mirroring on the TP Link was straightforward, if inelegant (you get what you pay for).  One gotcha is that I had to select the mirroring port a second time before it 'stuck' (see previous parenthetical note).  I mirrored the port that connects the switch to my router, and after some futzing with my Raspberry Pi's external USB network adapter and an ancient USB hub, I was in business: This setup is temporary and I hope to neaten it up at some point in the near future. JA3 parses client Hello SSL/TLS packets and extracts out fields to create a string that can be used to uniquely identify a client (a fingerprint).  Instead of passing around a potentially large string, an MD5 hash is calculated.  Since I only needed to capture Hello packets, I did a quick Google to find a tcpdump filter that would drop the rest of the traffic coming over the mirror port:  tcpdump -ni eth1 "tcp port 443 and (tcp[((tcp[12] &amp; 0xf0) &gt;&gt; 2)] = 0x16)" This filter actually grabs server Hellos too, but I can live with that for now.  I let the capture run for about a day, and then I ran the JA3 python script on the capture file to generate the aforementioned hashes.  Even though the capture file was only 39MB, it took the script a while to process all of the packets.  At some point, I'd like to take a closer look at the script and see if there are any opportunities for optimization.  In order to make my life easier, I did add a JSON output option and submit a Pull Request.  Once I had my fingerprints, I compared them against a list that the trisul folks generated in late June from fingerprint strings that Lee Brotherston generated in late March.  Anyway, the point is that these fingerprints are not that recent - and it showed.  Of 82 unique fingerprints that were captured in my network, seven of them appeared in the trisul list.  None of the seven were Chrome which makes sense given the high release cadence that the Chrome team maintains.  I am curious what is lurking in those 75 remaining hashes.  My Nest?  Angry Birds?  Jinyang's fridge? So, the first thought I have is that for JA3 to be useful in a whitelisting approach, there is going to have to be a trusted source of fingerprints that is constantly profiling new versions of popular clients and generating fingerprints for them.  It would be nice to have a process that could monitor for new versions of applications like Chrome and Firefox, automatically pull them down, and generate signatures for them.  The second thought I had is that there might be less unique fingerprints to deal with in an enterprise setting (depending on how locked down the network is, of course) since there aren't going to be a bunch of random devices phoning home and mobile apps being used.]]></summary></entry><entry><title type="html">Shakes fist at Citi…</title><link href="http://0.0.0.0:4000/2016/10/31/shakes-fist-at-citi.html" rel="alternate" type="text/html" title="Shakes fist at Citi…" /><published>2016-10-31T01:50:16+00:00</published><updated>2016-10-31T01:50:16+00:00</updated><id>http://0.0.0.0:4000/2016/10/31/shakes-fist-at-citi</id><content type="html" xml:base="http://0.0.0.0:4000/2016/10/31/shakes-fist-at-citi.html"><![CDATA[<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body></p>
<p>EDIT: So as Kodiologist notes in the comment section, the same simple Javascript (substring()) is executed every time, so it's just as easy to evaluate it manually.  However, in early July of 2017, Citi upped their game and there is now obfuscated Javascript code that must be evaluated in order to successfully login.  The code requires DOM to be present, so I had to switch from node.js to phantom.js, but I've got something working.  I'll blog about it in a subsequent post.</p>
<p>As I mentioned in a previous blog post, I wrote a budgeting helper program in Python that scrapes purchases from my credit cards' sites and emails me a daily summary of how much I  have spent so far in the current month.  I had scrapers working for my two primary cards for several years and everything was hunky dory until Costco decided to switch from American Express to Citi.  Don't get me wrong, I think the switch was a good move overall, but it forced me to have to write yet another scraper.</p>
<p>Perl and Python have made screen scraping an easy, if not enjoyable, task with their HTTP client and regex libraries.  My first real screen scraping endeavor was creating an AIM bot in Perl that could provide fantasy football stats.  This was during the era of pay-for-live-stats, so being able to get free stats was compelling for a skinflint like me.  The script scraped various sports sites to obtain the necessary data.  While I was ultimately able to build the bot, it was of limited utility to anyone but me due to fact that scoring is handled differently from league to league (points per yard, points per reception, etc.).  Also, most fantasy sites soon made live stats free.  Anyway, scraping unprotected sports sites is a straightforward process, but scraping credit card sites is a fair bit more complex.  As you would imagine, most of the difficulty stems from authenticating to the site.</p>
<p>While all credit card sites require a username and password to login, many require some additional authentication.  For example, Chase requires that you performed MFA at some point in the past on the machine.  It 'remembers' that you authenticated by planting a super cookie on your machine if Flash is present, or taking a fingerprint of your browser configuration if Flash is not installed.  One of these must be present when you attempt to login, or you are prompted to perform MFA again.  When I initially logged into the Citi site I was pleasantly surprised to see that there was not a secondary form of authentication (ordinarily, I'm a big fan of MFA and I activate it wherever possible, but it makes scraping a royal pain).  Based on what I was seeing in Firebug, it seemed like writing the Citi scraper would be straightforward.  I quickly hacked up my old Amex scraper to hit the Citi site, but I was not able to login.  I ran out of time, and I moved to Denver, so the scraper got put on the back burner.  After a few weeks in Denver, I realized I had no idea what our spending habits looked like, so I figured it was time to resurrect the scraper.</p>
<p>I took another look in Firebug, but I could not see what was preventing my script from logging in.  Eventually, I came across the Tamper Data add-on for Firefox that allows manipulation of a POST request before it is submitted to a remote site.  I removed POST fields one by one until I determine which ones were required for successfully logging in.  Two fields named XXX_Extra and XYZ_Extra turned out to be critical.  After some digging, I determined that these two fields were being populated by dynamically generated Javascript when the page loaded.  Apparently, Citi only wants browsers logging into their site, and ensuring that the client can execute Javascript is how they are enforcing this restriction.  At this point, I realized that I was either going to have to try and parse and evaluate Javascript from within Python, or I could shell out to a Javascript runtime and have it execute the Javascript for me.  Neither of these options seemed particularly fun, so I punted for a few more weeks.</p>
<p>Eventually, I found myself with a few free hours so I decided to tackle the Javascript issue.  I installed the execjs Python package that allows for Javascript code to be passed to an external Javascript runtime for execution.  I initially tried to use Googe's V8 for some reason, but it would not build on my Pi, and I've done enough debugging of esoteric g++ build errors for a lifetime (Thanks, <a href="https://shibboleth.net/">Shibboleth</a>!).  Then, I remembered that I got node to run on the Pi without too much difficulty a few years earlier.  I reinstalled node and voilà I was able to execute Javascript from my Python scripts.  The hard part was not yet done, as I still needed to figure out which functions in the dynamically generated mess of Javascript to call in order to populate XXX_Extra and XYZ_Extra properly.</p>
<p>In the blob of Javascript that Citi dynamically generates on the login page, there are four functions called by four jQuery(document).ready() functions (see below).  Each of these appears to manipulate XXX_Extra and XYZ_Extra, so it was unclear to me which function was setting the values that I needed to use, or if maybe they all needed to execute in a certain order.  I read a bit about the order of execution of ready() functions and tried to attack the problem from that angle.  Then, I realized that there was some bush-league misdirection going on within 3 of the 4 ready() functions.  Things like input.extrafie1d (note the 1 instead of the l).  So, all I really needed to do was:</p>
<ul>
<li>Scan through the Javascript until I found a ready() function  with the correctly named fields.  Let's call it legit_ready()</li>
<li>Find the function that legit_ready() is calling.  Let's call it legit_function().</li>
<li>Remove jQuery() usage from legit_function() and add a return statement so that we can retrieve XXX_Extra and XYZ_Extra.  Let's call the result legit_function_mod()</li>
<li>Use execjs to execute legit_function_mod().</li>
</ul>
<p>In the example below, tnmECSrrTlV2oA() is the legitimate function.</p>
<pre>function tnmECSrrTlV2oA() {
        var bcPY4RwbaIed1kgFGmN;
        var Cl9dJfqNEg5iVl44 = 1;
        var fXMHPy14iEBhe3ltWRyo = 'vYZc0Afe3ZgbkmE';
        var vZMo9w6L4lxuJZ4QqNQe = '5daf8e32e2be5e9';
        if (vZMo9w6L4lxuJZ4QqNQe != null &amp;&amp; vZMo9w6L4lxuJZ4QqNQe != "" &amp;&amp; vZMo9w6L4lxuJZ4QqNQe != "null") {
            var eF7UYGonVKYF2ThAMrU4 = '15';
            var RWzXGZXMxkXGBI8k33eC = '14';
            var C19dJfqNEg5iVl44 = vZMo9w6L4lxuJZ4QqNQe.substring(vZMo9w6L4lxuJZ4QqNQe.length - eF7UYGonVKYF2ThAMrU4);
            bcPY4RwbaIed1kgFGmN = '&lt;input type="hidden" class="extraField" name=XXX_Extra value='+ClH6VNEo33xrnjI1AMnL+'&gt;&lt;/input&gt;&lt;input type="hidden" class="extraField" name=XYZ_Extra value=d866bd6d6_4558b&gt;&lt;/input&gt;';
            jQuery("form").each(function () {
                if (isValidUrl(jQuery(this).attr("action"))) {
                    jQuery(this).append(bcPY4RwbaIed1kgFGmN);
                }
            });
        }
    }
jQuery(document).ready(function () {
    tnmECSrrTlV2oA();
    var version = parseFloat(jQuery.fn.jquery);
    if (version &lt; 1.7) {         if (version &gt;= 1.3 &amp;&amp; version &lt;= 1.4) {
            jQuery("form").live("submit", function () {
                jQuery("input.extraField").remove();
                tnmECSrrTlV2oA();
            });
        } else {
            jQuery(document).delegate("form", "submit", function () {
                jQuery("input.extraField").remove();
                tnmECSrrTlV2oA();
            });
        }
    } else {
        jQuery(document).on("submit", "form", function () {
            jQuery("input.extraField").remove();
            tnmECSrrTlV2oA();
        });
    }
});
function lPQda4j3csW() {
    var LoZLl5tpQOIevallbw5E;
    var LQiH2gqDylIyGfKC = 1;
    var fl1hcrZN1otRnBdwJ3r6 = 'cc3b4ZYbymBVX21';
    var GU5uy8zgPYs8M8DOMFVa = 'iD1XTHChxJhiIQz';
    if (GU5uy8zgPYs8M8DOMFVa != null &amp;&amp; GU5uy8zgPYs8M8DOMFVa != "" &amp;&amp; GU5uy8zgPYs8M8DOMFVa != "null") {
        var JwvoffgHHJKZQxv6 = '5';
        var Eyi2Ivyql09q88Yl = '8';
        var LQiH2gqDy1IyGfKC = GU5uy8zgPYs8M8DOMFVa.substring(GU5uy8zgPYs8M8DOMFVa.length - JwvoffgHHJKZQxv6);
        LoZL15tpQOIevallbw5E = '&lt;input type="hidden" class="extraField" name=XXX_Extra value='+nr1WTaykfq1eULQIu+'&gt;&lt;/input&gt;&lt;input type="hidden" class="extraField" name=XYZ_Extra value=d866bd6d6_4558b&gt;&lt;/input&gt;';
        jQuery("form").each(function () {
            if (isValidUrl(jQuery(this).attr("action"))) {
                jQuery(this).append(LoZLl5tpQOIevallbw5E);
            }
        });
    }
}
jQuery(document).ready(function () {
    lPQda4j3csW();
    var version = parseFloat(jQuery.fn.jquery);
    if (version &lt; 1.7) {         if (version &gt;= 1.3 &amp;&amp; version &lt;= 1.4) {
            jQuery("form").live("submit", function () {
                jQuery("input.extraFie1d").remove();
                lPQda4j3csW();
            });
        } else {
            jQuery(document).delegate("form", "submit", function () {
                jQuery("input.extraFie1d").remove();
                lPQda4j3csW();
            });
        }
    } else {
        jQuery(document).on("submit", "form", function () {
            jQuery("input.extraFie1d").remove();
            lPQda4j3csW();
        });
    }
});
function addExtraField() {
    var z60glaXFCqll4cgYRCgbI;
    var o2rIqWXlv0oVbBxTru = 1;
    var j84EiJ5AYqKwraeNS = 'WxDXBUgqzpwwGUQ';
    var oS4irOVQSSIa02PF3 = 'Nz50uj0X2PcnVqT';
    if (oS4irOVQSSIa02PF3 != null &amp;&amp; oS4irOVQSSIa02PF3 != "" &amp;&amp; oS4irOVQSSIa02PF3 != "null") {
        var LjoV0CN1rDIF1CZZ3 = '13';
        var lJdY2VxbKCdpuL1yV = '13';
        var o2rIqWX1v0oVbBxTru = oS4irOVQSSIa02PF3.substring(oS4irOVQSSIa02PF3.length - LjoV0CN1rDIF1CZZ3);
        z60g1aXFCqll4cgYRCgbI = '&lt;input type="hidden" class="extraField" name=XXX_Extra value='+EelRYB13TgG0CKlHpMFq+'&gt;&lt;/input&gt;&lt;input type="hidden" class="extraField" name=XYZ_Extra value=d866bd6d6_4558b&gt;&lt;/input&gt;';
        jQuery("form").each(function () {
            if (isValidUrl(jQuery(this).attr("action"))) {
                jQuery(this).append(z60glaXFCqll4cgYRCgbI);
            }
        });
    }
}
jQuery(document).ready(function () {
    addExtraField();
    var version = parseFloat(jQuery.fn.jquery);
    if (version &lt; 1.7) {         if (version &gt;= 1.3 &amp;&amp; version &lt;= 1.4) {
            jQuery("form").live("submit", function () {
                jQuery("input.extraFie1d").remove();
                addExtraField();
            });
        } else {
            jQuery(document).delegate("form", "submit", function () {
                jQuery("input.extraFie1d").remove();
                addExtraField();
            });
        }
    } else {
        jQuery(document).on("submit", "form", function () {
            jQuery("input.extraFie1d").remove();
            addExtraField();
        });
    }
});
function uozum7FnwvHv() {
    var ZiFNClZ0w7jkl34AchYkq;
    var kCbq3BNCYlEllvBfqx = 1;
    var B6vo9Q0Q1YDKhINPODIj = 'w3Cj6Epk2w3LEwV';
    var USL8UK2p0QdhaKFAptdL = 'x926nd9tKPtxJvG';
    if (USL8UK2p0QdhaKFAptdL != null &amp;&amp; USL8UK2p0QdhaKFAptdL != "" &amp;&amp; USL8UK2p0QdhaKFAptdL != "null") {
        var uvj4KlzfimISwXTXi928 = '5';
        var hrop06OmPDdHHWrUSZdg = '8';
        var kCbq3BNCY1EllvBfqx = USL8UK2p0QdhaKFAptdL.substring(USL8UK2p0QdhaKFAptdL.length - uvj4KlzfimISwXTXi928);
        ZiFNClZ0w7jk134AchYkq = ='&lt;input type="hidden" class="extraField" name=XXX_Extra value='+y98k3T6ILvLsT01wWgXVC+'&gt;&lt;/input&gt;&lt;input type="hidden" class="extraField" name=XYZ_Extra value=d866bd6d6_4558b&gt;&lt;/input&gt;';
        jQuery("form").each(function () {
            if (isValidUrl(jQuery(this).attr("action"))) {
                jQuery(this).append(ZiFNClZ0w7jkl34AchYkq);
            }
        });
    }
}
jQuery(document).ready(function () {
    uozum7FnwvHv();
    var version = parseFloat(jQuery.fn.jquery);
    if (version &lt; 1.7) {         if (version &gt;= 1.3 &amp;&amp; version &lt;= 1.4) {
            jQuery("form").live("submit", function () {
                jQuery("input.extraFie1d").remove();
                uozum7FnwvHv();
            });
        } else {
            jQuery(document).delegate("form", "submit", function () {
                jQuery("input.extraFie1d").remove();
                uozum7FnwvHv();
            });
        }
    } else {
        jQuery(document).on("submit", "form", function () {
            jQuery("input.extraFie1d").remove();
            uozum7FnwvHv();
        });
    }
});</pre>
<p>I wish I could say that I quickly finished up the scraper after solving the Javascript problem, but alas there was a lot more regex writing, poking at JSON and fighting with cron that had to be done.  However, that stuff is not particularly interesting, so I'll spare you the gory details.</body></html></p>]]></content><author><name></name></author><summary type="html"><![CDATA[EDIT: So as Kodiologist notes in the comment section, the same simple Javascript (substring()) is executed every time, so it's just as easy to evaluate it manually.  However, in early July of 2017, Citi upped their game and there is now obfuscated Javascript code that must be evaluated in order to successfully login.  The code requires DOM to be present, so I had to switch from node.js to phantom.js, but I've got something working.  I'll blog about it in a subsequent post. As I mentioned in a previous blog post, I wrote a budgeting helper program in Python that scrapes purchases from my credit cards' sites and emails me a daily summary of how much I  have spent so far in the current month.  I had scrapers working for my two primary cards for several years and everything was hunky dory until Costco decided to switch from American Express to Citi.  Don't get me wrong, I think the switch was a good move overall, but it forced me to have to write yet another scraper. Perl and Python have made screen scraping an easy, if not enjoyable, task with their HTTP client and regex libraries.  My first real screen scraping endeavor was creating an AIM bot in Perl that could provide fantasy football stats.  This was during the era of pay-for-live-stats, so being able to get free stats was compelling for a skinflint like me.  The script scraped various sports sites to obtain the necessary data.  While I was ultimately able to build the bot, it was of limited utility to anyone but me due to fact that scoring is handled differently from league to league (points per yard, points per reception, etc.).  Also, most fantasy sites soon made live stats free.  Anyway, scraping unprotected sports sites is a straightforward process, but scraping credit card sites is a fair bit more complex.  As you would imagine, most of the difficulty stems from authenticating to the site. While all credit card sites require a username and password to login, many require some additional authentication.  For example, Chase requires that you performed MFA at some point in the past on the machine.  It 'remembers' that you authenticated by planting a super cookie on your machine if Flash is present, or taking a fingerprint of your browser configuration if Flash is not installed.  One of these must be present when you attempt to login, or you are prompted to perform MFA again.  When I initially logged into the Citi site I was pleasantly surprised to see that there was not a secondary form of authentication (ordinarily, I'm a big fan of MFA and I activate it wherever possible, but it makes scraping a royal pain).  Based on what I was seeing in Firebug, it seemed like writing the Citi scraper would be straightforward.  I quickly hacked up my old Amex scraper to hit the Citi site, but I was not able to login.  I ran out of time, and I moved to Denver, so the scraper got put on the back burner.  After a few weeks in Denver, I realized I had no idea what our spending habits looked like, so I figured it was time to resurrect the scraper. I took another look in Firebug, but I could not see what was preventing my script from logging in.  Eventually, I came across the Tamper Data add-on for Firefox that allows manipulation of a POST request before it is submitted to a remote site.  I removed POST fields one by one until I determine which ones were required for successfully logging in.  Two fields named XXX_Extra and XYZ_Extra turned out to be critical.  After some digging, I determined that these two fields were being populated by dynamically generated Javascript when the page loaded.  Apparently, Citi only wants browsers logging into their site, and ensuring that the client can execute Javascript is how they are enforcing this restriction.  At this point, I realized that I was either going to have to try and parse and evaluate Javascript from within Python, or I could shell out to a Javascript runtime and have it execute the Javascript for me.  Neither of these options seemed particularly fun, so I punted for a few more weeks. Eventually, I found myself with a few free hours so I decided to tackle the Javascript issue.  I installed the execjs Python package that allows for Javascript code to be passed to an external Javascript runtime for execution.  I initially tried to use Googe's V8 for some reason, but it would not build on my Pi, and I've done enough debugging of esoteric g++ build errors for a lifetime (Thanks, Shibboleth!).  Then, I remembered that I got node to run on the Pi without too much difficulty a few years earlier.  I reinstalled node and voilà I was able to execute Javascript from my Python scripts.  The hard part was not yet done, as I still needed to figure out which functions in the dynamically generated mess of Javascript to call in order to populate XXX_Extra and XYZ_Extra properly. In the blob of Javascript that Citi dynamically generates on the login page, there are four functions called by four jQuery(document).ready() functions (see below).  Each of these appears to manipulate XXX_Extra and XYZ_Extra, so it was unclear to me which function was setting the values that I needed to use, or if maybe they all needed to execute in a certain order.  I read a bit about the order of execution of ready() functions and tried to attack the problem from that angle.  Then, I realized that there was some bush-league misdirection going on within 3 of the 4 ready() functions.  Things like input.extrafie1d (note the 1 instead of the l).  So, all I really needed to do was: Scan through the Javascript until I found a ready() function  with the correctly named fields.  Let's call it legit_ready() Find the function that legit_ready() is calling.  Let's call it legit_function(). Remove jQuery() usage from legit_function() and add a return statement so that we can retrieve XXX_Extra and XYZ_Extra.  Let's call the result legit_function_mod() Use execjs to execute legit_function_mod(). In the example below, tnmECSrrTlV2oA() is the legitimate function. function tnmECSrrTlV2oA() { var bcPY4RwbaIed1kgFGmN; var Cl9dJfqNEg5iVl44 = 1; var fXMHPy14iEBhe3ltWRyo = 'vYZc0Afe3ZgbkmE'; var vZMo9w6L4lxuJZ4QqNQe = '5daf8e32e2be5e9'; if (vZMo9w6L4lxuJZ4QqNQe != null &amp;&amp; vZMo9w6L4lxuJZ4QqNQe != "" &amp;&amp; vZMo9w6L4lxuJZ4QqNQe != "null") { var eF7UYGonVKYF2ThAMrU4 = '15'; var RWzXGZXMxkXGBI8k33eC = '14'; var C19dJfqNEg5iVl44 = vZMo9w6L4lxuJZ4QqNQe.substring(vZMo9w6L4lxuJZ4QqNQe.length - eF7UYGonVKYF2ThAMrU4); bcPY4RwbaIed1kgFGmN = '&lt;input type="hidden" class="extraField" name=XXX_Extra value='+ClH6VNEo33xrnjI1AMnL+'&gt;&lt;/input&gt;&lt;input type="hidden" class="extraField" name=XYZ_Extra value=d866bd6d6_4558b&gt;&lt;/input&gt;'; jQuery("form").each(function () { if (isValidUrl(jQuery(this).attr("action"))) { jQuery(this).append(bcPY4RwbaIed1kgFGmN); } }); } } jQuery(document).ready(function () { tnmECSrrTlV2oA(); var version = parseFloat(jQuery.fn.jquery); if (version &lt; 1.7) { if (version &gt;= 1.3 &amp;&amp; version &lt;= 1.4) { jQuery("form").live("submit", function () { jQuery("input.extraField").remove(); tnmECSrrTlV2oA(); }); } else { jQuery(document).delegate("form", "submit", function () { jQuery("input.extraField").remove(); tnmECSrrTlV2oA(); }); } } else { jQuery(document).on("submit", "form", function () { jQuery("input.extraField").remove(); tnmECSrrTlV2oA(); }); } }); function lPQda4j3csW() { var LoZLl5tpQOIevallbw5E; var LQiH2gqDylIyGfKC = 1; var fl1hcrZN1otRnBdwJ3r6 = 'cc3b4ZYbymBVX21'; var GU5uy8zgPYs8M8DOMFVa = 'iD1XTHChxJhiIQz'; if (GU5uy8zgPYs8M8DOMFVa != null &amp;&amp; GU5uy8zgPYs8M8DOMFVa != "" &amp;&amp; GU5uy8zgPYs8M8DOMFVa != "null") { var JwvoffgHHJKZQxv6 = '5'; var Eyi2Ivyql09q88Yl = '8'; var LQiH2gqDy1IyGfKC = GU5uy8zgPYs8M8DOMFVa.substring(GU5uy8zgPYs8M8DOMFVa.length - JwvoffgHHJKZQxv6); LoZL15tpQOIevallbw5E = '&lt;input type="hidden" class="extraField" name=XXX_Extra value='+nr1WTaykfq1eULQIu+'&gt;&lt;/input&gt;&lt;input type="hidden" class="extraField" name=XYZ_Extra value=d866bd6d6_4558b&gt;&lt;/input&gt;'; jQuery("form").each(function () { if (isValidUrl(jQuery(this).attr("action"))) { jQuery(this).append(LoZLl5tpQOIevallbw5E); } }); } } jQuery(document).ready(function () { lPQda4j3csW(); var version = parseFloat(jQuery.fn.jquery); if (version &lt; 1.7) { if (version &gt;= 1.3 &amp;&amp; version &lt;= 1.4) { jQuery("form").live("submit", function () { jQuery("input.extraFie1d").remove(); lPQda4j3csW(); }); } else { jQuery(document).delegate("form", "submit", function () { jQuery("input.extraFie1d").remove(); lPQda4j3csW(); }); } } else { jQuery(document).on("submit", "form", function () { jQuery("input.extraFie1d").remove(); lPQda4j3csW(); }); } }); function addExtraField() { var z60glaXFCqll4cgYRCgbI; var o2rIqWXlv0oVbBxTru = 1; var j84EiJ5AYqKwraeNS = 'WxDXBUgqzpwwGUQ'; var oS4irOVQSSIa02PF3 = 'Nz50uj0X2PcnVqT'; if (oS4irOVQSSIa02PF3 != null &amp;&amp; oS4irOVQSSIa02PF3 != "" &amp;&amp; oS4irOVQSSIa02PF3 != "null") { var LjoV0CN1rDIF1CZZ3 = '13'; var lJdY2VxbKCdpuL1yV = '13'; var o2rIqWX1v0oVbBxTru = oS4irOVQSSIa02PF3.substring(oS4irOVQSSIa02PF3.length - LjoV0CN1rDIF1CZZ3); z60g1aXFCqll4cgYRCgbI = '&lt;input type="hidden" class="extraField" name=XXX_Extra value='+EelRYB13TgG0CKlHpMFq+'&gt;&lt;/input&gt;&lt;input type="hidden" class="extraField" name=XYZ_Extra value=d866bd6d6_4558b&gt;&lt;/input&gt;'; jQuery("form").each(function () { if (isValidUrl(jQuery(this).attr("action"))) { jQuery(this).append(z60glaXFCqll4cgYRCgbI); } }); } } jQuery(document).ready(function () { addExtraField(); var version = parseFloat(jQuery.fn.jquery); if (version &lt; 1.7) { if (version &gt;= 1.3 &amp;&amp; version &lt;= 1.4) { jQuery("form").live("submit", function () { jQuery("input.extraFie1d").remove(); addExtraField(); }); } else { jQuery(document).delegate("form", "submit", function () { jQuery("input.extraFie1d").remove(); addExtraField(); }); } } else { jQuery(document).on("submit", "form", function () { jQuery("input.extraFie1d").remove(); addExtraField(); }); } }); function uozum7FnwvHv() { var ZiFNClZ0w7jkl34AchYkq; var kCbq3BNCYlEllvBfqx = 1; var B6vo9Q0Q1YDKhINPODIj = 'w3Cj6Epk2w3LEwV'; var USL8UK2p0QdhaKFAptdL = 'x926nd9tKPtxJvG'; if (USL8UK2p0QdhaKFAptdL != null &amp;&amp; USL8UK2p0QdhaKFAptdL != "" &amp;&amp; USL8UK2p0QdhaKFAptdL != "null") { var uvj4KlzfimISwXTXi928 = '5'; var hrop06OmPDdHHWrUSZdg = '8'; var kCbq3BNCY1EllvBfqx = USL8UK2p0QdhaKFAptdL.substring(USL8UK2p0QdhaKFAptdL.length - uvj4KlzfimISwXTXi928); ZiFNClZ0w7jk134AchYkq = ='&lt;input type="hidden" class="extraField" name=XXX_Extra value='+y98k3T6ILvLsT01wWgXVC+'&gt;&lt;/input&gt;&lt;input type="hidden" class="extraField" name=XYZ_Extra value=d866bd6d6_4558b&gt;&lt;/input&gt;'; jQuery("form").each(function () { if (isValidUrl(jQuery(this).attr("action"))) { jQuery(this).append(ZiFNClZ0w7jkl34AchYkq); } }); } } jQuery(document).ready(function () { uozum7FnwvHv(); var version = parseFloat(jQuery.fn.jquery); if (version &lt; 1.7) { if (version &gt;= 1.3 &amp;&amp; version &lt;= 1.4) { jQuery("form").live("submit", function () { jQuery("input.extraFie1d").remove(); uozum7FnwvHv(); }); } else { jQuery(document).delegate("form", "submit", function () { jQuery("input.extraFie1d").remove(); uozum7FnwvHv(); }); } } else { jQuery(document).on("submit", "form", function () { jQuery("input.extraFie1d").remove(); uozum7FnwvHv(); }); } }); I wish I could say that I quickly finished up the scraper after solving the Javascript problem, but alas there was a lot more regex writing, poking at JSON and fighting with cron that had to be done.  However, that stuff is not particularly interesting, so I'll spare you the gory details.]]></summary></entry><entry><title type="html">Using Confluent’s JDBC Connector without installing the entire platform</title><link href="http://0.0.0.0:4000/2016/05/02/using-confluents-jdbc-connector-without-installing-the-entire-platform.html" rel="alternate" type="text/html" title="Using Confluent’s JDBC Connector without installing the entire platform" /><published>2016-05-02T20:00:08+00:00</published><updated>2016-05-02T20:00:08+00:00</updated><id>http://0.0.0.0:4000/2016/05/02/using-confluents-jdbc-connector-without-installing-the-entire-platform</id><content type="html" xml:base="http://0.0.0.0:4000/2016/05/02/using-confluents-jdbc-connector-without-installing-the-entire-platform.html"><![CDATA[<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body></p>
<p>I was interested in trying out <a href="http://docs.confluent.io/2.0.0/connect/connect-jdbc/docs/index.html">Confluent's JDBC connector</a> without installing their entire platform (I'd like to stick to vanilla Kafka as much as possible).  Here are the steps I followed to get it working with SQL Server.</p>
<p>Download <a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/0.9.0.1/kafka_2.10-0.9.0.1.tgz">Kafka 0.9</a>, untar the archive, and create a directory named connect_libs in the kafka root (<code>kafka_2.10-0.9.0.1/connect_libs</code>).</p>
<p>Download the <a href="http://packages.confluent.io/archive/2.0/confluent-2.0.1-2.11.7.zip">Confluent platform</a> and extract the following jars (you should also be able to pull these from Confluent's Maven repo, though I was unsuccessful):</p>
<ul>
<li>common-config-2.0.1.jar</li>
<li>common-metrics-2.0.1.jar</li>
<li>common-utils-2.0.1.jar</li>
<li>kafka-connect-jdbc-2.0.1.jar</li>
</ul>
<p>*Place these jars along with the <a href="https://msdn.microsoft.com/en-us/library/mt484311%28v=sql.110%29.aspx">SQL Server</a> driver in <code>kafka_2.10-0.9.0.1/connect_libs</code>. Update <code>bootstrap.servers</code> in <code>kafka_2.10-0.9.0.1/config/connect-standalone.properties</code> with the broker list and create <code>kafka_2.10-0.9.0.1/config/connect-jdbc.properties</code> with the settings to try out:</p>
<pre>name=sqlserver-feed
connector.class=io.confluent.connect.jdbc.JdbcSourceConnector
tasks.max=1

connection.url=jdbc:sqlserver://xxx.xxx.xxx.xxx:1433;databaseName=FeedDB;user=user;password=password
table.whitelist=tblFeedIP,tblFeedURL

mode=timestamp+incrementing
timestamp.column.name=localLastUpdated
incrementing.column.name=id

topic.prefix=stg-</pre>
<p>Create the topics <code>stg-tblFeedIP</code> and <code>stg-tblFeedURL</code> on the cluster.</p>
<p>Add the connect_libs directory to the classpath:<br />
<code>export CLASSPATH="connect_libs/*"</code></p>
<p>And finally, run the connector in standalone mode with (make sure you are in the root kafka directory): <code>bin/connect-standalone.sh config/connect-standalone.properties config/connect-jdbc.properties</code></p>
<p>Then, tail your topics to verify that messages are being produced by the connector.</p>
<p>* If you don't care about cluttering up the default libs directory (<code>kafka_2.10-0.9.0.1/libs</code>), you can also just dump the jars there and not have to worry about setting the classpath.</body></html></p>]]></content><author><name></name></author><summary type="html"><![CDATA[I was interested in trying out Confluent's JDBC connector without installing their entire platform (I'd like to stick to vanilla Kafka as much as possible).  Here are the steps I followed to get it working with SQL Server. Download Kafka 0.9, untar the archive, and create a directory named connect_libs in the kafka root (kafka_2.10-0.9.0.1/connect_libs). Download the Confluent platform and extract the following jars (you should also be able to pull these from Confluent's Maven repo, though I was unsuccessful): common-config-2.0.1.jar common-metrics-2.0.1.jar common-utils-2.0.1.jar kafka-connect-jdbc-2.0.1.jar *Place these jars along with the SQL Server driver in kafka_2.10-0.9.0.1/connect_libs. Update bootstrap.servers in kafka_2.10-0.9.0.1/config/connect-standalone.properties with the broker list and create kafka_2.10-0.9.0.1/config/connect-jdbc.properties with the settings to try out: name=sqlserver-feed connector.class=io.confluent.connect.jdbc.JdbcSourceConnector tasks.max=1]]></summary></entry><entry><title type="html">Fixing MoCA interference with cable modems</title><link href="http://0.0.0.0:4000/networking/2015/04/25/fixing-moca-interference-with-cable-modems.html" rel="alternate" type="text/html" title="Fixing MoCA interference with cable modems" /><published>2015-04-25T19:00:51+00:00</published><updated>2015-04-25T19:00:51+00:00</updated><id>http://0.0.0.0:4000/networking/2015/04/25/fixing-moca-interference-with-cable-modems</id><content type="html" xml:base="http://0.0.0.0:4000/networking/2015/04/25/fixing-moca-interference-with-cable-modems.html"><![CDATA[<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body>
<p>In order to connect my home theater setup in the family room with the computers in my office and basement, I am using <a href="http://en.wikipedia.org/wiki/Multimedia_over_Coax_Alliance">MoCA </a>Ethernet to coaxial cable bridges. Verizon Fios customers will be familiar with MoCA as it's how Verizon DVRs communicate with Verizon-provided Access Points to get listings, and it's how multi-room DVRs communicate. Unfortunately, MoCA does not play well with my Cox cable modem service despite supposedly using different frequencies.  I got things to a liveable state by using a diplexer to isolate the MoCA adapater.  However, after a recent upgrade to a DOCSIS 3.0 modem, I began getting periodic disconnects of both my cable modem and my MoCA adapters.  Near as I can tell, the MoCA signals were causing the cable modem to reboot, and the reboot process caused the MoCA adapters to lose connectivity.  I was pulling my hair out (what's left of it, anyway) trying to figure out a way for MoCA and my cable modem to happily coexist on the same piece of coax. After some Googling, I came across a <a href="http://www.cisco.com/c/dam/en/us/td/docs/video/at_home/Cable_Accessories/4031235_B.pdf">PDF</a> from our friends at Cisco that describes how to eliminate interference between MoCA and cable modems (page 12). All you need to do is place a MoCA Point-of-Entry filter between your cable modem and the wall. As the name suggests, PoE filters are generally placed in the junction box that connects your house to the utility poll and are designed to block MoCA frequencies so that your signal does not escape your house and so that your neighbors' signals do not enter. However, they also do a great job of keep the MoCA frequencies from DoSing your cable modem.</p>
<p></body></html></p>]]></content><author><name></name></author><category term="Networking" /><summary type="html"><![CDATA[In order to connect my home theater setup in the family room with the computers in my office and basement, I am using MoCA Ethernet to coaxial cable bridges. Verizon Fios customers will be familiar with MoCA as it's how Verizon DVRs communicate with Verizon-provided Access Points to get listings, and it's how multi-room DVRs communicate. Unfortunately, MoCA does not play well with my Cox cable modem service despite supposedly using different frequencies.  I got things to a liveable state by using a diplexer to isolate the MoCA adapater.  However, after a recent upgrade to a DOCSIS 3.0 modem, I began getting periodic disconnects of both my cable modem and my MoCA adapters.  Near as I can tell, the MoCA signals were causing the cable modem to reboot, and the reboot process caused the MoCA adapters to lose connectivity.  I was pulling my hair out (what's left of it, anyway) trying to figure out a way for MoCA and my cable modem to happily coexist on the same piece of coax. After some Googling, I came across a PDF from our friends at Cisco that describes how to eliminate interference between MoCA and cable modems (page 12). All you need to do is place a MoCA Point-of-Entry filter between your cable modem and the wall. As the name suggests, PoE filters are generally placed in the junction box that connects your house to the utility poll and are designed to block MoCA frequencies so that your signal does not escape your house and so that your neighbors' signals do not enter. However, they also do a great job of keep the MoCA frequencies from DoSing your cable modem.]]></summary></entry><entry><title type="html">&amp;amp;nbsp; nuisance</title><link href="http://0.0.0.0:4000/programming/2013/04/07/nuisance.html" rel="alternate" type="text/html" title="&amp;amp;nbsp; nuisance" /><published>2013-04-07T20:26:15+00:00</published><updated>2013-04-07T20:26:15+00:00</updated><id>http://0.0.0.0:4000/programming/2013/04/07/nuisance</id><content type="html" xml:base="http://0.0.0.0:4000/programming/2013/04/07/nuisance.html"><![CDATA[<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body></p>
<p dir="ltr">Recently, one of my former developers had an issue with &amp;nbsp; (Non-breaking space) on one of the websites he was working on.  This particular dev has a knack for finding annoying problems that are seemingly simple, but take an inordinate amount of time to solve.  The issue arose when he obtained the innerHTML of an element and then attempted to determine if the content contained a space.  If there was a space, innerHTML returned &amp;nbsp; as a string, so logically he decided to use an if-statement checking for &amp;nbsp;:</p>
<p dir="ltr"><code>if (input == "&amp;nbsp;")</code></p>
<p>This did not work for some reason, so he asked me to take a look at the code.  After a lot of discussion, and trying things out in Firebug and JSFiddle, I finally asked him to view source on the page and tell me what he saw.  It turns out that the custom framework that his company uses automatically replaces any occurrence of  &amp;nbsp; with the Unicode equivalent, &amp; #xa0; (Note: there should not be a space between the &amp; and the #.  Fittingly for this post, Wordpress is converting it into a space, regardless of how I attempt to escape it.).  Off the top of my I head I could not think of a better workaround than to create a variable with &amp;nbsp; in it, and then use the variable in the comparison:</p>
<p dir="ltr"><code>var thisisdumb = "&amp;nb" + "sp;"<br />
<code>if (input == thisisdumb)</code><br />
</code></p>
<p dir="ltr">That worked, but it felt distinctly unsatisfying, and kind of dirty.  I told the dev to ask somebody if there was a way to escape &amp;nbsp; so that the framework would ignore it.</p>
<p></body></html></p>]]></content><author><name></name></author><category term="Programming" /><category term="HTML" /><category term="Javascript" /><summary type="html"><![CDATA[Recently, one of my former developers had an issue with &amp;nbsp; (Non-breaking space) on one of the websites he was working on.  This particular dev has a knack for finding annoying problems that are seemingly simple, but take an inordinate amount of time to solve.  The issue arose when he obtained the innerHTML of an element and then attempted to determine if the content contained a space.  If there was a space, innerHTML returned &amp;nbsp; as a string, so logically he decided to use an if-statement checking for &amp;nbsp;: if (input == "&amp;nbsp;") This did not work for some reason, so he asked me to take a look at the code.  After a lot of discussion, and trying things out in Firebug and JSFiddle, I finally asked him to view source on the page and tell me what he saw.  It turns out that the custom framework that his company uses automatically replaces any occurrence of  &amp;nbsp; with the Unicode equivalent, &amp; #xa0; (Note: there should not be a space between the &amp; and the #. Fittingly for this post, Wordpress is converting it into a space, regardless of how I attempt to escape it.).  Off the top of my I head I could not think of a better workaround than to create a variable with &amp;nbsp; in it, and then use the variable in the comparison: var thisisdumb = "&amp;nb" + "sp;" if (input == thisisdumb) That worked, but it felt distinctly unsatisfying, and kind of dirty.  I told the dev to ask somebody if there was a way to escape &amp;nbsp; so that the framework would ignore it.]]></summary></entry><entry><title type="html">Quick KBS Review</title><link href="http://0.0.0.0:4000/2013/04/03/kbs-review.html" rel="alternate" type="text/html" title="Quick KBS Review" /><published>2013-04-03T03:37:00+00:00</published><updated>2013-04-03T03:37:00+00:00</updated><id>http://0.0.0.0:4000/2013/04/03/kbs-review</id><content type="html" xml:base="http://0.0.0.0:4000/2013/04/03/kbs-review.html"><![CDATA[<p><!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"><br />
<html><body>
<p>Just shared a KBS with Urn. As you'd imagine, Bourbon was the most noticeable aroma immediately after opening. It was milder than BCBS or Parabola, however. We let it warm up for about 10 minutes and then took a few sips. I was surprised how muted the Bourbon flavor was compared to the other barrel-aged stouts that we had recently tried. Unsurprisingly, it reminded me of FBS with the hints of coffee and chocolate. It's like Founders took Breakfast Stout, tweaked it slightly and then added Bourbon undertones throughout. It's definitely smooth and balanced, and I don't think it would benefit much from cellaring. After a year or two, the Bourbon flavor might disappear entirely. I have the 2012 vintage that Urn got me for my birthday last year, and I'll probably drink it sooner than later. My current ratings are KBS &gt; Parabola &gt; BCBS &gt; Dark Hollow. That said, Parabola and BCBS aged for a year or two might trump KBS.</p>
<p></body></html></p>]]></content><author><name></name></author><category term="Beer" /><category term="founders" /><category term="kbs" /><summary type="html"><![CDATA[Just shared a KBS with Urn. As you'd imagine, Bourbon was the most noticeable aroma immediately after opening. It was milder than BCBS or Parabola, however. We let it warm up for about 10 minutes and then took a few sips. I was surprised how muted the Bourbon flavor was compared to the other barrel-aged stouts that we had recently tried. Unsurprisingly, it reminded me of FBS with the hints of coffee and chocolate. It's like Founders took Breakfast Stout, tweaked it slightly and then added Bourbon undertones throughout. It's definitely smooth and balanced, and I don't think it would benefit much from cellaring. After a year or two, the Bourbon flavor might disappear entirely. I have the 2012 vintage that Urn got me for my birthday last year, and I'll probably drink it sooner than later. My current ratings are KBS &gt; Parabola &gt; BCBS &gt; Dark Hollow. That said, Parabola and BCBS aged for a year or two might trump KBS.]]></summary></entry></feed>